Project: Personal AI Assistant
- Goal: Build a chatbot that can answer questions from personal notes using LLMs.
- Stack: Python, FAISS, sentence-transformers, OpenAI/Ollama
- Status: MVP ready, tested with DeepSeek-Coder and Mistral

Meeting Notes - March 10
- Discussed embedding strategies: chunk size should be 300â€“500 tokens
- Need to evaluate local inference speed vs OpenAI latency
- Set target for GUI prototype by end of next week

Todo List
- [x] Add config.yaml for model switching
- [x] Make local and remote LLMs pluggable
- [ ] Test PDF support
- [ ] Add Whisper integration for voice input

Tech Notes
- FAISS is used for fast nearest-neighbor search in embedding space
- SentenceTransformers like 'all-MiniLM-L6-v2' are lightweight and fast
- Ollama makes it easy to run models like Llama2 or DeepSeek locally
- OpenAI GPT-3.5 is faster but less private compared to local LLMs

Personal Goals
- Learn LangChain and RAG in more depth
- Build a Chrome extension version of the assistant
- Explore use cases for therapists and journaling

Useful Prompts
- "Summarize my goals from the past month."
- "What's the status of the AI assistant project?"
- "Remind me of embedding size recommendations."

Research Ideas
- Use LLMs to help process dream journals or therapy notes
- Compare RAG pipelines: Haystack vs LangChain vs custom Python
